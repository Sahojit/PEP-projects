{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ End-to-End Data Engineering Pipeline\n",
    "## Production-Grade Cryptocurrency Analytics Platform\n",
    "\n",
    "**Author**: Data Engineering Portfolio Project  \n",
    "**Architecture**: Medallion (Bronze ‚Üí Silver ‚Üí Gold)  \n",
    "**Data Source**: CoinGecko Public API  \n",
    "**Storage**: SQLite Analytical Warehouse  \n",
    "\n",
    "---\n",
    "\n",
    "### üìã Pipeline Overview\n",
    "\n",
    "This project demonstrates a complete data engineering workflow:\n",
    "\n",
    "1. **Data Ingestion** - Continuous API data collection\n",
    "2. **Bronze Layer** - Raw data storage (no modifications)\n",
    "3. **Silver Layer** - Data cleaning, validation, deduplication\n",
    "4. **Gold Layer** - Analytics-ready aggregations\n",
    "5. **Statistics Engine** - Advanced metrics computation\n",
    "6. **Visualization** - Business intelligence dashboards\n",
    "7. **Orchestration** - Automated pipeline execution\n",
    "8. **Streaming Simulation** - Continuous data processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter Server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31mTraceback (most recent call last):\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/notebook/traittypes.py\", line 235, in _resolve_classes\n",
      "\u001b[1;31m    klass = self._resolve_string(klass)\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 2015, in _resolve_string\n",
      "\u001b[1;31m    return import_item(string)\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/traitlets/utils/importstring.py\", line 33, in import_item\n",
      "\u001b[1;31m    module = __import__(package, fromlist=[obj])\n",
      "\u001b[1;31mModuleNotFoundError: No module named 'jupyter_server.contents'\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mTraceback (most recent call last):\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/bin/jupyter-notebook\", line 8, in <module>\n",
      "\u001b[1;31m    sys.exit(main())\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/jupyter_core/application.py\", line 283, in launch_instance\n",
      "\u001b[1;31m    super().launch_instance(argv=argv, **kwargs)\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1073, in launch_instance\n",
      "\u001b[1;31m    app = cls.instance(**kwargs)\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/traitlets/config/configurable.py\", line 583, in instance\n",
      "\u001b[1;31m    inst = cls(*args, **kwargs)\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1292, in __new__\n",
      "\u001b[1;31m    inst.setup_instance(*args, **kwargs)\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1335, in setup_instance\n",
      "\u001b[1;31m    super(HasTraits, self).setup_instance(*args, **kwargs)\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/traitlets/traitlets.py\", line 1311, in setup_instance\n",
      "\u001b[1;31m    init(self)\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/notebook/traittypes.py\", line 226, in instance_init\n",
      "\u001b[1;31m    self._resolve_classes()\n",
      "\u001b[1;31m  File \"/Users/sahojitkarmakar/Library/Python/3.9/lib/python/site-packages/notebook/traittypes.py\", line 238, in _resolve_classes\n",
      "\u001b[1;31m    warn(f\"{klass} is not importable. Is it installed?\", ImportWarning)\n",
      "\u001b[1;31mTypeError: warn() missing 1 required keyword-only argument: 'stacklevel'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 2. Configuration & Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Configuration\n",
    "class PipelineConfig:\n",
    "    \"\"\"Central configuration for the data pipeline\"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    API_BASE_URL = \"https://api.coingecko.com/api/v3\"\n",
    "    CRYPTO_IDS = [\"bitcoin\", \"ethereum\", \"cardano\", \"solana\", \"polkadot\"]\n",
    "    VS_CURRENCY = \"usd\"\n",
    "    API_TIMEOUT = 10\n",
    "    \n",
    "    # Database Configuration\n",
    "    DB_PATH = \"crypto_analytics.db\"\n",
    "    \n",
    "    # Table Names (Medallion Architecture)\n",
    "    BRONZE_TABLE = \"bronze_raw_prices\"\n",
    "    SILVER_TABLE = \"silver_cleaned_prices\"\n",
    "    GOLD_TABLE = \"gold_analytics\"\n",
    "    \n",
    "    # Processing Configuration\n",
    "    BATCH_SIZE = 100\n",
    "    ROLLING_WINDOW = 5  # for rolling averages\n",
    "    \n",
    "    # Streaming Simulation\n",
    "    INGESTION_INTERVAL = 10  # seconds between API calls\n",
    "    MAX_ITERATIONS = 20  # for demo purposes\n",
    "\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('DataPipeline')\n",
    "logger.info(\"üéØ Pipeline configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è 3. Database Initialization\n",
    "\n",
    "Setting up SQLite warehouse with Medallion Architecture tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_database() -> None:\n",
    "    \"\"\"\n",
    "    Initialize SQLite database with Bronze, Silver, and Gold layer tables.\n",
    "    \n",
    "    Bronze: Raw data with full API response\n",
    "    Silver: Cleaned, validated, deduplicated data\n",
    "    Gold: Aggregated analytics-ready data\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Bronze Layer - Raw ingestion\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {PipelineConfig.BRONZE_TABLE} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            crypto_id TEXT NOT NULL,\n",
    "            price REAL,\n",
    "            market_cap REAL,\n",
    "            volume_24h REAL,\n",
    "            price_change_24h REAL,\n",
    "            ingestion_timestamp TEXT NOT NULL,\n",
    "            raw_json TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Silver Layer - Cleaned data\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {PipelineConfig.SILVER_TABLE} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            crypto_id TEXT NOT NULL,\n",
    "            price REAL NOT NULL,\n",
    "            market_cap REAL NOT NULL,\n",
    "            volume_24h REAL NOT NULL,\n",
    "            price_change_24h REAL,\n",
    "            ingestion_timestamp TEXT NOT NULL,\n",
    "            processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(crypto_id, ingestion_timestamp)\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Gold Layer - Analytics\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {PipelineConfig.GOLD_TABLE} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            crypto_id TEXT NOT NULL,\n",
    "            avg_price REAL,\n",
    "            min_price REAL,\n",
    "            max_price REAL,\n",
    "            std_price REAL,\n",
    "            total_volume REAL,\n",
    "            avg_market_cap REAL,\n",
    "            data_points INTEGER,\n",
    "            calculation_timestamp TEXT NOT NULL,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    logger.info(\"‚úÖ Database initialized with Bronze, Silver, and Gold tables\")\n",
    "\n",
    "\n",
    "# Initialize the database\n",
    "initialize_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•â 4. Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "Fetch live cryptocurrency data from CoinGecko API and store as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_crypto_prices() -> Optional[List[Dict]]:\n",
    "    \"\"\"\n",
    "    Fetch current cryptocurrency prices from CoinGecko API.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing price data, or None on failure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"{PipelineConfig.API_BASE_URL}/simple/price\"\n",
    "        params = {\n",
    "            'ids': ','.join(PipelineConfig.CRYPTO_IDS),\n",
    "            'vs_currencies': PipelineConfig.VS_CURRENCY,\n",
    "            'include_market_cap': 'true',\n",
    "            'include_24hr_vol': 'true',\n",
    "            'include_24hr_change': 'true'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params, timeout=PipelineConfig.API_TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        logger.info(f\"‚úÖ Successfully fetched data for {len(data)} cryptocurrencies\")\n",
    "        return data\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"‚ùå API request failed: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Unexpected error in data fetch: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def ingest_to_bronze(data: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Insert raw API data into Bronze layer without any transformation.\n",
    "    \n",
    "    Args:\n",
    "        data: Raw API response dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Number of records inserted\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        logger.warning(\"‚ö†Ô∏è No data to ingest to Bronze layer\")\n",
    "        return 0\n",
    "    \n",
    "    conn = sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    ingestion_time = datetime.now().isoformat()\n",
    "    records_inserted = 0\n",
    "    \n",
    "    for crypto_id, metrics in data.items():\n",
    "        try:\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT INTO {PipelineConfig.BRONZE_TABLE} \n",
    "                (crypto_id, price, market_cap, volume_24h, price_change_24h, \n",
    "                 ingestion_timestamp, raw_json)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                crypto_id,\n",
    "                metrics.get(f'{PipelineConfig.VS_CURRENCY}'),\n",
    "                metrics.get(f'{PipelineConfig.VS_CURRENCY}_market_cap'),\n",
    "                metrics.get(f'{PipelineConfig.VS_CURRENCY}_24h_vol'),\n",
    "                metrics.get(f'{PipelineConfig.VS_CURRENCY}_24h_change'),\n",
    "                ingestion_time,\n",
    "                json.dumps(metrics)\n",
    "            ))\n",
    "            records_inserted += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to insert {crypto_id} to Bronze: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    logger.info(f\"‚úÖ Bronze Layer: Inserted {records_inserted} raw records\")\n",
    "    return records_inserted\n",
    "\n",
    "\n",
    "# Test the ingestion\n",
    "test_data = fetch_crypto_prices()\n",
    "if test_data:\n",
    "    ingest_to_bronze(test_data)\n",
    "    print(\"\\nüìä Sample Bronze Layer Data:\")\n",
    "    df_bronze = pd.read_sql_query(\n",
    "        f\"SELECT * FROM {PipelineConfig.BRONZE_TABLE} LIMIT 5\", \n",
    "        sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    )\n",
    "    print(df_bronze[['crypto_id', 'price', 'market_cap', 'ingestion_timestamp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•à 5. Silver Layer - Data Cleaning & Validation\n",
    "\n",
    "Clean, validate, and deduplicate data from Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_record(record: pd.Series) -> bool:\n",
    "    \"\"\"\n",
    "    Validate a single data record against business rules.\n",
    "    \n",
    "    Validation rules:\n",
    "    - Price must be positive\n",
    "    - Market cap must be positive\n",
    "    - Volume must be non-negative\n",
    "    - No null values in critical fields\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(record['price']) or record['price'] <= 0:\n",
    "            return False\n",
    "        if pd.isna(record['market_cap']) or record['market_cap'] <= 0:\n",
    "            return False\n",
    "        if pd.isna(record['volume_24h']) or record['volume_24h'] < 0:\n",
    "            return False\n",
    "        if pd.isna(record['crypto_id']) or pd.isna(record['ingestion_timestamp']):\n",
    "            return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def clean_and_load_silver() -> int:\n",
    "    \"\"\"\n",
    "    Process Bronze layer data: clean, validate, deduplicate, and load to Silver.\n",
    "    \n",
    "    Returns:\n",
    "        Number of records successfully processed to Silver layer\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    \n",
    "    # Read unprocessed Bronze records\n",
    "    query = f\"\"\"\n",
    "        SELECT DISTINCT b.* \n",
    "        FROM {PipelineConfig.BRONZE_TABLE} b\n",
    "        LEFT JOIN {PipelineConfig.SILVER_TABLE} s \n",
    "            ON b.crypto_id = s.crypto_id \n",
    "            AND b.ingestion_timestamp = s.ingestion_timestamp\n",
    "        WHERE s.id IS NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    df_bronze = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    if df_bronze.empty:\n",
    "        logger.info(\"‚ÑπÔ∏è No new Bronze records to process\")\n",
    "        conn.close()\n",
    "        return 0\n",
    "    \n",
    "    logger.info(f\"üîÑ Processing {len(df_bronze)} Bronze records\")\n",
    "    \n",
    "    # Data Quality Checks\n",
    "    initial_count = len(df_bronze)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df_bronze.drop_duplicates(\n",
    "        subset=['crypto_id', 'ingestion_timestamp'], \n",
    "        keep='first'\n",
    "    )\n",
    "    duplicates_removed = initial_count - len(df_clean)\n",
    "    \n",
    "    # Validate records\n",
    "    df_clean['is_valid'] = df_clean.apply(validate_record, axis=1)\n",
    "    df_valid = df_clean[df_clean['is_valid']].copy()\n",
    "    invalid_records = len(df_clean) - len(df_valid)\n",
    "    \n",
    "    # Handle nulls in optional fields\n",
    "    df_valid['price_change_24h'] = df_valid['price_change_24h'].fillna(0)\n",
    "    \n",
    "    # Insert into Silver layer\n",
    "    records_inserted = 0\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for _, row in df_valid.iterrows():\n",
    "        try:\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT OR IGNORE INTO {PipelineConfig.SILVER_TABLE}\n",
    "                (crypto_id, price, market_cap, volume_24h, price_change_24h, ingestion_timestamp)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                row['crypto_id'],\n",
    "                row['price'],\n",
    "                row['market_cap'],\n",
    "                row['volume_24h'],\n",
    "                row['price_change_24h'],\n",
    "                row['ingestion_timestamp']\n",
    "            ))\n",
    "            records_inserted += cursor.rowcount\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to insert to Silver: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    logger.info(f\"‚úÖ Silver Layer: Processed {records_inserted} records\")\n",
    "    logger.info(f\"   üìä Duplicates removed: {duplicates_removed}\")\n",
    "    logger.info(f\"   üìä Invalid records filtered: {invalid_records}\")\n",
    "    \n",
    "    return records_inserted\n",
    "\n",
    "\n",
    "# Test Silver layer processing\n",
    "clean_and_load_silver()\n",
    "print(\"\\nüìä Sample Silver Layer Data:\")\n",
    "df_silver = pd.read_sql_query(\n",
    "    f\"SELECT * FROM {PipelineConfig.SILVER_TABLE} ORDER BY processed_at DESC LIMIT 5\",\n",
    "    sqlite3.connect(PipelineConfig.DB_PATH)\n",
    ")\n",
    "print(df_silver[['crypto_id', 'price', 'market_cap', 'volume_24h']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•á 6. Gold Layer - Analytics Aggregations\n",
    "\n",
    "Create business-ready analytics tables with aggregated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gold_analytics() -> int:\n",
    "    \"\"\"\n",
    "    Compute aggregated analytics from Silver layer and store in Gold layer.\n",
    "    \n",
    "    Metrics computed per cryptocurrency:\n",
    "    - Average, min, max, std deviation of price\n",
    "    - Total trading volume\n",
    "    - Average market capitalization\n",
    "    - Number of data points\n",
    "    \n",
    "    Returns:\n",
    "        Number of analytics records created\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    \n",
    "    # Read Silver layer data\n",
    "    df_silver = pd.read_sql_query(\n",
    "        f\"SELECT * FROM {PipelineConfig.SILVER_TABLE}\",\n",
    "        conn\n",
    "    )\n",
    "    \n",
    "    if df_silver.empty:\n",
    "        logger.warning(\"‚ö†Ô∏è No Silver data available for analytics\")\n",
    "        conn.close()\n",
    "        return 0\n",
    "    \n",
    "    # Compute aggregations per cryptocurrency\n",
    "    analytics = df_silver.groupby('crypto_id').agg({\n",
    "        'price': ['mean', 'min', 'max', 'std'],\n",
    "        'volume_24h': 'sum',\n",
    "        'market_cap': 'mean',\n",
    "        'id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    analytics.columns = [\n",
    "        'crypto_id', 'avg_price', 'min_price', 'max_price', 'std_price',\n",
    "        'total_volume', 'avg_market_cap', 'data_points'\n",
    "    ]\n",
    "    \n",
    "    # Handle NaN in std (occurs when only 1 data point)\n",
    "    analytics['std_price'] = analytics['std_price'].fillna(0)\n",
    "    \n",
    "    # Insert into Gold layer\n",
    "    cursor = conn.cursor()\n",
    "    calculation_time = datetime.now().isoformat()\n",
    "    records_inserted = 0\n",
    "    \n",
    "    for _, row in analytics.iterrows():\n",
    "        try:\n",
    "            cursor.execute(f\"\"\"\n",
    "                INSERT INTO {PipelineConfig.GOLD_TABLE}\n",
    "                (crypto_id, avg_price, min_price, max_price, std_price,\n",
    "                 total_volume, avg_market_cap, data_points, calculation_timestamp)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                row['crypto_id'],\n",
    "                row['avg_price'],\n",
    "                row['min_price'],\n",
    "                row['max_price'],\n",
    "                row['std_price'],\n",
    "                row['total_volume'],\n",
    "                row['avg_market_cap'],\n",
    "                row['data_points'],\n",
    "                calculation_time\n",
    "            ))\n",
    "            records_inserted += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to insert Gold analytics: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    logger.info(f\"‚úÖ Gold Layer: Created {records_inserted} analytics records\")\n",
    "    return records_inserted\n",
    "\n",
    "\n",
    "# Test Gold layer\n",
    "compute_gold_analytics()\n",
    "print(\"\\nüìä Sample Gold Layer Analytics:\")\n",
    "df_gold = pd.read_sql_query(\n",
    "    f\"SELECT * FROM {PipelineConfig.GOLD_TABLE} ORDER BY created_at DESC LIMIT 5\",\n",
    "    sqlite3.connect(PipelineConfig.DB_PATH)\n",
    ")\n",
    "print(df_gold[['crypto_id', 'avg_price', 'min_price', 'max_price', 'std_price', 'data_points']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 7. Advanced Statistics Engine\n",
    "\n",
    "Compute sophisticated metrics: rolling averages, returns, correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advanced_statistics() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute advanced statistical metrics on Silver layer data.\n",
    "    \n",
    "    Metrics:\n",
    "    - Rolling averages (configurable window)\n",
    "    - Percentage returns\n",
    "    - Price volatility (std deviation)\n",
    "    - Cross-asset correlation matrix\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with enriched statistics\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    \n",
    "    # Load Silver data with proper timestamp parsing\n",
    "    df = pd.read_sql_query(\n",
    "        f\"SELECT * FROM {PipelineConfig.SILVER_TABLE} ORDER BY ingestion_timestamp\",\n",
    "        conn\n",
    "    )\n",
    "    conn.close()\n",
    "    \n",
    "    if df.empty or len(df) < 2:\n",
    "        logger.warning(\"‚ö†Ô∏è Insufficient data for advanced statistics\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df['ingestion_timestamp'] = pd.to_datetime(df['ingestion_timestamp'])\n",
    "    \n",
    "    # Compute statistics per cryptocurrency\n",
    "    results = []\n",
    "    \n",
    "    for crypto_id in df['crypto_id'].unique():\n",
    "        crypto_df = df[df['crypto_id'] == crypto_id].sort_values('ingestion_timestamp').copy()\n",
    "        \n",
    "        if len(crypto_df) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Rolling average\n",
    "        crypto_df['rolling_avg'] = crypto_df['price'].rolling(\n",
    "            window=min(PipelineConfig.ROLLING_WINDOW, len(crypto_df)),\n",
    "            min_periods=1\n",
    "        ).mean()\n",
    "        \n",
    "        # Percentage returns\n",
    "        crypto_df['returns_pct'] = crypto_df['price'].pct_change() * 100\n",
    "        \n",
    "        # Volatility (rolling std)\n",
    "        crypto_df['volatility'] = crypto_df['price'].rolling(\n",
    "            window=min(PipelineConfig.ROLLING_WINDOW, len(crypto_df)),\n",
    "            min_periods=1\n",
    "        ).std()\n",
    "        \n",
    "        results.append(crypto_df)\n",
    "    \n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df_enriched = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Computed advanced statistics for {df_enriched['crypto_id'].nunique()} assets\")\n",
    "    \n",
    "    return df_enriched\n",
    "\n",
    "\n",
    "def compute_correlation_matrix() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute price correlation matrix across all cryptocurrencies.\n",
    "    \n",
    "    Returns:\n",
    "        Correlation matrix DataFrame or None if insufficient data\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    \n",
    "    df = pd.read_sql_query(\n",
    "        f\"SELECT crypto_id, price, ingestion_timestamp FROM {PipelineConfig.SILVER_TABLE}\",\n",
    "        conn\n",
    "    )\n",
    "    conn.close()\n",
    "    \n",
    "    if df.empty:\n",
    "        return None\n",
    "    \n",
    "    # Pivot to get prices per crypto over time\n",
    "    df_pivot = df.pivot_table(\n",
    "        index='ingestion_timestamp',\n",
    "        columns='crypto_id',\n",
    "        values='price',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    if df_pivot.shape[0] < 2 or df_pivot.shape[1] < 2:\n",
    "        logger.warning(\"‚ö†Ô∏è Insufficient data for correlation matrix\")\n",
    "        return None\n",
    "    \n",
    "    corr_matrix = df_pivot.corr()\n",
    "    \n",
    "    logger.info(f\"‚úÖ Computed correlation matrix for {len(corr_matrix)} assets\")\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "# Test statistics computation\n",
    "df_stats = compute_advanced_statistics()\n",
    "if not df_stats.empty:\n",
    "    print(\"\\nüìä Advanced Statistics Sample:\")\n",
    "    print(df_stats[['crypto_id', 'price', 'rolling_avg', 'returns_pct', 'volatility']].head(10))\n",
    "\n",
    "corr_matrix = compute_correlation_matrix()\n",
    "if corr_matrix is not None:\n",
    "    print(\"\\nüìä Price Correlation Matrix:\")\n",
    "    print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 8. Visualization Dashboard\n",
    "\n",
    "Create comprehensive visualizations for data insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(df_stats: pd.DataFrame, corr_matrix: Optional[pd.DataFrame]) -> None:\n",
    "    \"\"\"\n",
    "    Generate comprehensive visualization dashboard.\n",
    "    \n",
    "    Visualizations:\n",
    "    1. Price trends over time\n",
    "    2. Rolling averages\n",
    "    3. Percentage returns\n",
    "    4. Correlation heatmap\n",
    "    \n",
    "    Args:\n",
    "        df_stats: DataFrame with computed statistics\n",
    "        corr_matrix: Correlation matrix DataFrame\n",
    "    \"\"\"\n",
    "    if df_stats.empty:\n",
    "        logger.warning(\"‚ö†Ô∏è No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = GridSpec(3, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Price Trends\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    for crypto_id in df_stats['crypto_id'].unique():\n",
    "        crypto_data = df_stats[df_stats['crypto_id'] == crypto_id]\n",
    "        ax1.plot(\n",
    "            crypto_data['ingestion_timestamp'],\n",
    "            crypto_data['price'],\n",
    "            marker='o',\n",
    "            label=crypto_id.capitalize(),\n",
    "            linewidth=2\n",
    "        )\n",
    "    ax1.set_title('Cryptocurrency Price Trends', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Time', fontsize=11)\n",
    "    ax1.set_ylabel('Price (USD)', fontsize=11)\n",
    "    ax1.legend(loc='best')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Rolling Averages\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    for crypto_id in df_stats['crypto_id'].unique():\n",
    "        crypto_data = df_stats[df_stats['crypto_id'] == crypto_id]\n",
    "        ax2.plot(\n",
    "            crypto_data['ingestion_timestamp'],\n",
    "            crypto_data['rolling_avg'],\n",
    "            marker='s',\n",
    "            label=crypto_id.capitalize(),\n",
    "            linewidth=2,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax2.set_title(f'Rolling Average (Window={PipelineConfig.ROLLING_WINDOW})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Time', fontsize=10)\n",
    "    ax2.set_ylabel('Rolling Avg Price (USD)', fontsize=10)\n",
    "    ax2.legend(loc='best', fontsize=8)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Percentage Returns\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    for crypto_id in df_stats['crypto_id'].unique():\n",
    "        crypto_data = df_stats[df_stats['crypto_id'] == crypto_id]\n",
    "        ax3.plot(\n",
    "            crypto_data['ingestion_timestamp'],\n",
    "            crypto_data['returns_pct'],\n",
    "            marker='D',\n",
    "            label=crypto_id.capitalize(),\n",
    "            linewidth=1.5,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax3.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax3.set_title('Percentage Returns', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('Time', fontsize=10)\n",
    "    ax3.set_ylabel('Returns (%)', fontsize=10)\n",
    "    ax3.legend(loc='best', fontsize=8)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Correlation Heatmap\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    if corr_matrix is not None and not corr_matrix.empty:\n",
    "        im = ax4.imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "        ax4.set_xticks(range(len(corr_matrix.columns)))\n",
    "        ax4.set_yticks(range(len(corr_matrix.index)))\n",
    "        ax4.set_xticklabels([col.capitalize() for col in corr_matrix.columns], rotation=45)\n",
    "        ax4.set_yticklabels([idx.capitalize() for idx in corr_matrix.index])\n",
    "        \n",
    "        # Add correlation values\n",
    "        for i in range(len(corr_matrix.index)):\n",
    "            for j in range(len(corr_matrix.columns)):\n",
    "                text = ax4.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax4, label='Correlation Coefficient')\n",
    "        ax4.set_title('Price Correlation Matrix', fontsize=12, fontweight='bold')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Insufficient data for correlation matrix',\n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.axis('off')\n",
    "    \n",
    "    plt.suptitle('Cryptocurrency Analytics Dashboard', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(\"‚úÖ Visualizations generated successfully\")\n",
    "\n",
    "\n",
    "# Generate visualizations\n",
    "if not df_stats.empty:\n",
    "    create_visualizations(df_stats, corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 9. Pipeline Orchestration\n",
    "\n",
    "Master function to execute the complete end-to-end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(visualize: bool = True) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Execute the complete data engineering pipeline.\n",
    "    \n",
    "    Pipeline stages:\n",
    "    1. Fetch data from API\n",
    "    2. Ingest to Bronze layer (raw)\n",
    "    3. Clean and load to Silver layer\n",
    "    4. Compute Gold layer analytics\n",
    "    5. Calculate advanced statistics\n",
    "    6. Generate visualizations (optional)\n",
    "    \n",
    "    Args:\n",
    "        visualize: Whether to generate visualizations\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with record counts per stage\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"üöÄ STARTING DATA PIPELINE EXECUTION\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    pipeline_stats = {\n",
    "        'bronze_records': 0,\n",
    "        'silver_records': 0,\n",
    "        'gold_records': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Data Ingestion\n",
    "        logger.info(\"\\nüì• Stage 1: Data Ingestion\")\n",
    "        raw_data = fetch_crypto_prices()\n",
    "        \n",
    "        if not raw_data:\n",
    "            logger.error(\"‚ùå Pipeline failed: No data fetched\")\n",
    "            return pipeline_stats\n",
    "        \n",
    "        # Stage 2: Bronze Layer\n",
    "        logger.info(\"\\nü•â Stage 2: Bronze Layer (Raw Storage)\")\n",
    "        pipeline_stats['bronze_records'] = ingest_to_bronze(raw_data)\n",
    "        \n",
    "        # Stage 3: Silver Layer\n",
    "        logger.info(\"\\nü•à Stage 3: Silver Layer (Cleaning & Validation)\")\n",
    "        pipeline_stats['silver_records'] = clean_and_load_silver()\n",
    "        \n",
    "        # Stage 4: Gold Layer\n",
    "        logger.info(\"\\nü•á Stage 4: Gold Layer (Analytics)\")\n",
    "        pipeline_stats['gold_records'] = compute_gold_analytics()\n",
    "        \n",
    "        # Stage 5: Advanced Statistics\n",
    "        logger.info(\"\\nüìä Stage 5: Advanced Statistics\")\n",
    "        df_stats = compute_advanced_statistics()\n",
    "        corr_matrix = compute_correlation_matrix()\n",
    "        \n",
    "        # Stage 6: Visualization\n",
    "        if visualize and not df_stats.empty:\n",
    "            logger.info(\"\\nüìà Stage 6: Generating Visualizations\")\n",
    "            create_visualizations(df_stats, corr_matrix)\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"‚úÖ PIPELINE EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"üìä Pipeline Statistics:\")\n",
    "        logger.info(f\"   Bronze records: {pipeline_stats['bronze_records']}\")\n",
    "        logger.info(f\"   Silver records: {pipeline_stats['silver_records']}\")\n",
    "        logger.info(f\"   Gold records: {pipeline_stats['gold_records']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Pipeline failed with error: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    return pipeline_stats\n",
    "\n",
    "\n",
    "# Execute the pipeline\n",
    "stats = run_pipeline(visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ 10. Continuous Ingestion Simulation\n",
    "\n",
    "Simulate streaming data pipeline with periodic execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_streaming_pipeline(iterations: int = None, interval: int = None) -> None:\n",
    "    \"\"\"\n",
    "    Simulate continuous data ingestion and processing.\n",
    "    \n",
    "    This function runs the pipeline in a loop, simulating a real-time\n",
    "    streaming data pipeline that processes data incrementally.\n",
    "    \n",
    "    Args:\n",
    "        iterations: Number of iterations (default from config)\n",
    "        interval: Seconds between iterations (default from config)\n",
    "    \"\"\"\n",
    "    iterations = iterations or PipelineConfig.MAX_ITERATIONS\n",
    "    interval = interval or PipelineConfig.INGESTION_INTERVAL\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"üåä STARTING STREAMING PIPELINE SIMULATION\")\n",
    "    logger.info(f\"   Iterations: {iterations}\")\n",
    "    logger.info(f\"   Interval: {interval} seconds\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    total_stats = {\n",
    "        'total_bronze': 0,\n",
    "        'total_silver': 0,\n",
    "        'total_gold': 0\n",
    "    }\n",
    "    \n",
    "    for i in range(1, iterations + 1):\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"üîÑ Iteration {i}/{iterations}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        # Run pipeline without visualization for intermediate iterations\n",
    "        visualize = (i == iterations)  # Only visualize on last iteration\n",
    "        stats = run_pipeline(visualize=visualize)\n",
    "        \n",
    "        # Accumulate statistics\n",
    "        total_stats['total_bronze'] += stats['bronze_records']\n",
    "        total_stats['total_silver'] += stats['silver_records']\n",
    "        total_stats['total_gold'] += stats['gold_records']\n",
    "        \n",
    "        # Wait before next iteration (except on last iteration)\n",
    "        if i < iterations:\n",
    "            logger.info(f\"\\n‚è≥ Waiting {interval} seconds before next iteration...\")\n",
    "            time.sleep(interval)\n",
    "    \n",
    "    # Final summary\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"üéâ STREAMING PIPELINE SIMULATION COMPLETED\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"üìä Total Records Processed:\")\n",
    "    logger.info(f\"   Bronze: {total_stats['total_bronze']}\")\n",
    "    logger.info(f\"   Silver: {total_stats['total_silver']}\")\n",
    "    logger.info(f\"   Gold: {total_stats['total_gold']}\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Show final database state\n",
    "    conn = sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    print(\"\\nüìä Final Database State:\")\n",
    "    \n",
    "    for table in [PipelineConfig.BRONZE_TABLE, PipelineConfig.SILVER_TABLE, PipelineConfig.GOLD_TABLE]:\n",
    "        count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0]['count']\n",
    "        print(f\"   {table}: {count} records\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Run streaming simulation\n",
    "# Uncomment the line below to run continuous ingestion\n",
    "# WARNING: This will make multiple API calls and take several minutes\n",
    "\n",
    "# run_streaming_pipeline(iterations=5, interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 11. Data Quality & Monitoring\n",
    "\n",
    "Query and inspect pipeline health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pipeline_report() -> None:\n",
    "    \"\"\"\n",
    "    Generate comprehensive pipeline health and data quality report.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(PipelineConfig.DB_PATH)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä DATA PIPELINE HEALTH REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Record counts per layer\n",
    "    print(\"\\nüóÑÔ∏è RECORD COUNTS BY LAYER:\")\n",
    "    for table in [PipelineConfig.BRONZE_TABLE, PipelineConfig.SILVER_TABLE, PipelineConfig.GOLD_TABLE]:\n",
    "        count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0]['count']\n",
    "        print(f\"   {table.upper()}: {count:,} records\")\n",
    "    \n",
    "    # Data quality metrics\n",
    "    print(\"\\n‚úÖ DATA QUALITY METRICS:\")\n",
    "    \n",
    "    # Bronze to Silver conversion rate\n",
    "    bronze_count = pd.read_sql_query(\n",
    "        f\"SELECT COUNT(*) as count FROM {PipelineConfig.BRONZE_TABLE}\", conn\n",
    "    ).iloc[0]['count']\n",
    "    \n",
    "    silver_count = pd.read_sql_query(\n",
    "        f\"SELECT COUNT(*) as count FROM {PipelineConfig.SILVER_TABLE}\", conn\n",
    "    ).iloc[0]['count']\n",
    "    \n",
    "    if bronze_count > 0:\n",
    "        quality_rate = (silver_count / bronze_count) * 100\n",
    "        print(f\"   Data Quality Rate: {quality_rate:.2f}%\")\n",
    "        print(f\"   Records Filtered: {bronze_count - silver_count}\")\n",
    "    \n",
    "    # Latest data timestamps\n",
    "    print(\"\\nüïê LATEST DATA TIMESTAMPS:\")\n",
    "    for table in [PipelineConfig.BRONZE_TABLE, PipelineConfig.SILVER_TABLE]:\n",
    "        latest = pd.read_sql_query(\n",
    "            f\"SELECT MAX(ingestion_timestamp) as latest FROM {table}\", conn\n",
    "        ).iloc[0]['latest']\n",
    "        print(f\"   {table.upper()}: {latest}\")\n",
    "    \n",
    "    # Asset coverage\n",
    "    print(\"\\nüí∞ ASSET COVERAGE:\")\n",
    "    assets = pd.read_sql_query(\n",
    "        f\"SELECT crypto_id, COUNT(*) as records FROM {PipelineConfig.SILVER_TABLE} GROUP BY crypto_id\",\n",
    "        conn\n",
    "    )\n",
    "    for _, row in assets.iterrows():\n",
    "        print(f\"   {row['crypto_id'].capitalize()}: {row['records']} data points\")\n",
    "    \n",
    "    # Latest analytics\n",
    "    print(\"\\nüìà LATEST ANALYTICS (GOLD LAYER):\")\n",
    "    gold_latest = pd.read_sql_query(\n",
    "        f\"\"\"\n",
    "        SELECT crypto_id, avg_price, min_price, max_price, std_price, data_points\n",
    "        FROM {PipelineConfig.GOLD_TABLE}\n",
    "        WHERE id IN (\n",
    "            SELECT MAX(id) FROM {PipelineConfig.GOLD_TABLE} GROUP BY crypto_id\n",
    "        )\n",
    "        ORDER BY crypto_id\n",
    "        \"\"\",\n",
    "        conn\n",
    "    )\n",
    "    \n",
    "    if not gold_latest.empty:\n",
    "        print(gold_latest.to_string(index=False))\n",
    "    else:\n",
    "        print(\"   No analytics data available yet\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "# Generate report\n",
    "generate_pipeline_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 12. Usage Instructions & Next Steps\n",
    "\n",
    "### Quick Start Guide\n",
    "\n",
    "```python\n",
    "# 1. Run single pipeline execution\n",
    "run_pipeline(visualize=True)\n",
    "\n",
    "# 2. Run streaming simulation (5 iterations, 10 seconds apart)\n",
    "run_streaming_pipeline(iterations=5, interval=10)\n",
    "\n",
    "# 3. Generate health report\n",
    "generate_pipeline_report()\n",
    "```\n",
    "\n",
    "### Architecture Summary\n",
    "\n",
    "**Bronze Layer** (`bronze_raw_prices`):\n",
    "- Raw API data storage\n",
    "- No transformations\n",
    "- Includes full JSON response\n",
    "- Audit trail for debugging\n",
    "\n",
    "**Silver Layer** (`silver_cleaned_prices`):\n",
    "- Cleaned and validated data\n",
    "- Deduplication enforced\n",
    "- Schema validation\n",
    "- Production-ready quality\n",
    "\n",
    "**Gold Layer** (`gold_analytics`):\n",
    "- Aggregated business metrics\n",
    "- Analytics-ready format\n",
    "- Optimized for reporting\n",
    "- Historical snapshots\n",
    "\n",
    "### Key Features Demonstrated\n",
    "\n",
    "‚úÖ **Medallion Architecture** - Industry-standard data lakehouse pattern  \n",
    "‚úÖ **Data Quality Checks** - Validation, deduplication, null handling  \n",
    "‚úÖ **Incremental Processing** - Processes only new data  \n",
    "‚úÖ **Observability** - Comprehensive logging throughout  \n",
    "‚úÖ **Error Handling** - Graceful failures with recovery  \n",
    "‚úÖ **Modular Design** - Reusable, testable functions  \n",
    "‚úÖ **Statistics Engine** - Rolling averages, returns, correlations  \n",
    "‚úÖ **Visualization** - Business intelligence dashboards  \n",
    "‚úÖ **Orchestration** - Single master pipeline function  \n",
    "‚úÖ **Streaming Simulation** - Continuous data processing  \n",
    "\n",
    "### Portfolio Talking Points\n",
    "\n",
    "1. **Scalability**: Modular design allows easy addition of new data sources\n",
    "2. **Data Quality**: Multi-layer validation ensures high-quality analytics\n",
    "3. **Observability**: Logging enables monitoring and debugging\n",
    "4. **Best Practices**: Follows industry-standard Medallion Architecture\n",
    "5. **Production-Ready**: Error handling, incremental processing, idempotency\n",
    "\n",
    "### Potential Enhancements\n",
    "\n",
    "- Add Apache Airflow for scheduling\n",
    "- Implement data versioning (Delta Lake)\n",
    "- Add data lineage tracking\n",
    "- Create dbt models for transformations\n",
    "- Implement CDC (Change Data Capture)\n",
    "- Add data quality tests (Great Expectations)\n",
    "- Create alerting for pipeline failures\n",
    "- Add partitioning for large datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Data Engineering Portfolio Project  \n",
    "**License**: MIT  \n",
    "**Contact**: [Your Contact Info]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
